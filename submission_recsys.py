# -*- coding: utf-8 -*-
"""submission_recsys.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1QpTCGLSDP8cInY-3oAjyVvvPZjdk7etC
"""

!pip install opendatasets

"""# Data Collection

Dataset ini diambil dari [Kaggle Movielens Dataset](https://www.kaggle.com/datasets/ayushimishra2809/movielens-dataset)
"""

import opendatasets

opendatasets.download('https://www.kaggle.com/datasets/ayushimishra2809/movielens-dataset')

"""# Data Understanding

Terdapat dua file, yakni movies dan rating movies.

Digunakan `len` untuk mengetahui berapa banyak data yang dimiliki
"""

import pandas as pd

movies = pd.read_csv('movielens-dataset/movies.csv')
rating = pd.read_csv('movielens-dataset/ratings.csv')


print('Length of Movies dataset: ', len(movies))
print('Length of Ratings dataset: ', len(rating))

"""# Explanatory Data Analysis

## Univariate Data Analysis

### Movies Variable

Di bawah ini merupakan informasi mengenai dataset `movies`, yang mana terdapat informasi mengenai `movieId`, `title` dari movie, dan `genres` dari movie.
"""

movies

"""`info()` digunakan untuk mengetahui informasi terkait tipe data dan non-null data pada dataset `movies`."""

# movies information
movies.info()

"""`isna()` dilakukan untuk mengetahui apakah ada missing value pada dataset `movies`"""

movies.isna().sum()

"""Dataset `movies` memiliki banyak data 10,329 sample dengan 3 fitur"""

movies.shape

print('Banyak data: ', len(movies.movieId.unique()))
print('Judul movies yang ada: ', movies.title.unique())

"""### Ratings Variable

Di bawah ini merupakan informasi mengenai dataset `movies`, yang mana terdapat informasi mengenai `userId`, `movieId`, `rating` dari movie, dan `timestamp` dari movie.
"""

rating

"""`info()` digunakan untuk mengetahui informasi terkait tipe data dan non-null data pada dataset `rating`."""

# rating information
rating.info()

"""`describe()` digunakan untuk mengetahui informasi terkait mean, std, min, quartil, dan max.

Dari tabel yang didapatkan, dapat diketahui bahwa dataset `rating`, memiliki minimum rating 0.5 dan maksimum rating 5 yang dapat diberikan oleh penonton.
"""

# describe dataset
rating.describe()

print('Jumlah judul movie: ', len(movies.movieId.unique()))
print('Jumlah genre movie: ', len(movies.genres.unique()))
print('Jumlah sample rating movie: ', len(rating.userId.unique()))

"""`isna()` dilakukan untuk mengetahui apakah ada missing value pada dataset `rating`"""

rating.isna().sum()

"""# Data Preprocessing

## Menggabungkan Movie

Penggabungan dataset `movies` dan `rating` dilakukan untuk mendapatkan rekomendasi movie. Fungsi `concatenate` akan menggabungkan kedua dataset tersebut berdasarkan fitur `movieId` yang unik.
"""

import numpy as np

movies_full = np.concatenate((movies.movieId.unique(),
                              rating.movieId.unique()))

movies_full = np.sort(np.unique(movies_full))

print('Jumlah seluruh data movie berdasarkan movieId: ', len(movies_full))

"""## Jumlah User

Mendapatkan jumlah user berdasarkan `userId` yang unik dan kemudian mengurutkannya.
"""

user = rating.userId.unique()
user = np.sort(np.unique(user))

print('Jumlah seluruh user: ', len(user))

"""## Mengetahui Rating

Menggabungkan kedua dataset `rating` dan `movies` ke dalam variable `movie` berdasarkan `movieId`.
"""

movie_info = pd.concat([rating, movies])

movie = pd.merge(rating, movie_info, on = 'movieId', how = 'left')
movie

"""Dapat diketahui bahwa variable `movie` memiliki banyak missing value. `isnull` digunakan untuk mengetahui seberapa banyak missing value yang ada"""

movie.isnull().sum()

"""Menggabungkan `movie` berdasarkan fitur `movieId`"""

movie.groupby('movieId').sum()

"""## Menggabungkan data dengan fitur Movie

Menggabungkan dataset `rating` dan `movies` menjadi `all_movie` berdasarkan `movieId`
"""

all_movie = pd.merge(rating, movies, on = 'movieId', how = 'left')
all_movie

"""# Handling Missing Value

Mengetahui apakah ada dataset yang kosong (missing value). Dapat diketahui bahwa tidak ada dataset yang kosong
"""

all_movie.isnull().sum()

"""Melakukan pengurutan berdasarkan `movieId`"""

all_movie = all_movie.sort_values('movieId', ascending=True)
all_movie

"""Mengecek berapa jumlah movies berdasarkan `movieId`"""

len(all_movie.movieId.unique())

"""Mengecek jumlah data terduplikasi data terhadap `movieId`"""

all_movie['movieId'].duplicated().sum()

"""Membuang data terduplikasi berdasarkan `movieId`"""

all_movie = all_movie.drop_duplicates('movieId')
len(all_movie['movieId'])

"""Membuat dictionary terhadap dataset yang telah disiapkan sebelumnya"""

movie_fix = pd.DataFrame({'id': all_movie['movieId'],
                          'title': all_movie['title'],
                          'genre': all_movie['genres']})

movie_fix.head(3)

"""# Modeling

## Content Based Filtering

Content Based Filtering dilakukan dengan membuat algoritma untuk mengetahui rekomendasi berdasarkan _history_ user.

Digunakan `Tfidfvectorizer` untuk melakukan perhitungan _idf_ pada `genre` dan melakukan mapping array.
"""

from sklearn.feature_extraction.text import TfidfVectorizer

tfidf = TfidfVectorizer()
tfidf.fit(movie_fix['genre'])
tfidf.get_feature_names_out()

"""Melakukan `fit_transform` untuk mendapatkan bentuk matrix"""

tfidf_mtx = tfidf.fit_transform(movie_fix['genre'])
tfidf_mtx.shape

"""Hasil dari matrix yang sebelumnya dibuat dapat dilihat dengan `todense()`"""

tfidf_mtx.todense()

"""Melihat tf-idf untuk movie berdasarkan genre yang telah spesifik"""

tfidf_df = pd.DataFrame(tfidf_mtx.todense(),
                        columns=tfidf.get_feature_names_out(),
                        index=movie_fix.title).sample(24, axis = 1).sample(10, axis = 0)

tfidf_df

"""### Consine Similarity

Dilakukan cosine similarity untuk mengetahui similarity degree di antara movie
"""

from sklearn.metrics.pairwise import cosine_similarity

cosine_sim = cosine_similarity(tfidf_mtx)
cosine_sim

"""Membuat variable `cosine_dim_df` yang merupakan dataframe untuk melihat kesamaan matrix yang dimiliki dari setiap movie. Dapat dilihat sebagai contoh bahwa movie Red Road (2006) memiliki kesamaan sebesar 0.49 dengan movie Night and the City (1992)."""

consine_dim_df = pd.DataFrame(cosine_sim, index = movie_fix['title'], columns=movie_fix['title'])
print('Shape: ', consine_dim_df.shape)

consine_dim_df.sample(5, axis =1).sample(10, axis = 0)

"""Membuat fungsi `movie_recommendation` dengan parameter `movie_name` untuk mendapatkan rekomendasi berdasarkan movie yang ada."""

def movie_recommendation(movie_name):
    k = 5
    items = movie_fix[['title', 'genre']]
    index = consine_dim_df.loc[:, movie_name].to_numpy().argpartition(range(-1, -k, -1))
    closest = consine_dim_df.columns[index[-1: -(k+2): -1]]
    closest = closest.drop(movie_name, errors = 'ignore')
    return pd.DataFrame(closest).merge(items).head(k)

"""Kemudian, akan dilakukan pencarian untuk mengetahui rekomendasi movie yang mirip dengan `Toy Story (1995)`"""

movie_fix[movie_fix.title.eq('Toy Story (1995)')]

"""Dengan menggunakan fungsi `movie_recommendation` diketahui bahwa terdapat lima movie yang mirip dan direkomendasikan berdasarkan rating pengguna terhadap movie `Toy Story (1995)`"""

movie_recommendation('Toy Story (1995)')

"""## Collaborative Filtering"""

# Import library
import pandas as pd
import numpy as np
from zipfile import ZipFile
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from pathlib import Path
import matplotlib.pyplot as plt

rating.sample(5)

"""### Data Preparation

melakukan tahapan preprocessing dengan melakukan beberapa encoding terhadap `userId`
"""

# UserID menjadi unique list
user_id = rating['userId'].unique().tolist()

# Encoding userID
user_encoded = {x: i for i, x in enumerate(user_id)}

# Encoding angka ke ke userID
to_user = {i: x for i, x in enumerate(user_id)}

"""melakukan tahapan preprocessing dengan melakukan beberapa encoding terhadap fitur `movieId`"""

# MovieId menjadi list tanpa nilai yang sama
movie_id = rating['movieId'].unique().tolist()

# Proses encoding movieId
movie_encoded = {x: i for i, x in enumerate(movie_id)}

# Proses encoding angka ke movieId
to_movie = {i: x for i, x in enumerate(movie_id)}

# Mapping userId ke dataframe genres
rating['genres'] = rating['userId'].map(user_encoded)

# Mapping movieD ke dataframe movies
rating['movies'] = rating['movieId'].map(movie_encoded)

"""Dilakukan beberapa preprocessing tambahan seperti mengubah tipe menjadi float, mengecek nilai maksimum dan minimum."""

num_users = len(user_encoded)

num_movie = len(to_movie)

rating['ratings'] = rating['rating'].values.astype(np.float32)

min_rating = min(rating['rating'])

max_rating = max(rating['rating'])

print('Number of User: {}, Number of movie: {}, Min Rating: {}, Max Rating: {}'.format(
    num_users, num_movie, min_rating, max_rating
))

"""### Splitting Data Training dan Validasi"""

# Mengacak dataset
df = rating.sample(frac=1, random_state=42)
df

"""Membagi data dengan rasio 80:20 untuk data training dan validation"""

# Membuat variabel x untuk mencocokkan data genres  dan movies menjadi satu value
x = df[['genres', 'movies']].values

# Membuat variabel y untuk membuat ratings dari hasil
y = df['ratings'].apply(lambda x: (x - min_rating) / (max_rating - min_rating)).values

# Membagi menjadi 80% data train dan 20% data validasi
train_indices = int(0.8 * df.shape[0])
x_train, x_val, y_train, y_val = (
    x[:train_indices],
    x[train_indices:],
    y[:train_indices],
    y[train_indices:]
)

print(x, y)

"""Melakukan proses training"""

class RecommenderNet(tf.keras.Model):

  # Insialisasi fungsi
  def __init__(self, num_users, num_movie, embedding_size, **kwargs):
    super(RecommenderNet, self).__init__(**kwargs)
    self.num_users = num_users
    self.num_movie = num_movie
    self.embedding_size = embedding_size
    self.user_embedding = layers.Embedding( # layer embedding user
        num_users,
        embedding_size,
        embeddings_initializer = 'he_normal',
        embeddings_regularizer = keras.regularizers.l2(1e-6)
    )
    self.user_bias = layers.Embedding(num_users, 1) # layer embedding user bias
    self.movie_embedding = layers.Embedding( # layer embeddings movies
        num_movie,
        embedding_size,
        embeddings_initializer = 'he_normal',
        embeddings_regularizer = keras.regularizers.l2(1e-6)
    )
    self.movie_bias = layers.Embedding(num_movie, 1) # layer embedding movies bias

  def call(self, inputs):
    user_vector = self.user_embedding(inputs[:,0]) # memanggil layer embedding 1
    user_bias = self.user_bias(inputs[:, 0]) # memanggil layer embedding 2
    movie_vector = self.movie_embedding(inputs[:, 1]) # memanggil layer embedding 3
    movie_bias = self.movie_bias(inputs[:, 1]) # memanggil layer embedding 4

    dot_user_movie = tf.tensordot(user_vector, movie_vector, 2)

    x = dot_user_movie + user_bias + movie_bias

    return tf.nn.sigmoid(x) # activation sigmoid

"""### Evaluation

Melakukan proses compile terhadap model dengan menggunakan matriks evaluasi RMSE
"""

model = RecommenderNet(num_users, num_movie, 50) # inisialisasi model

# model compile
model.compile(
    loss = tf.keras.losses.BinaryCrossentropy(),
    optimizer = keras.optimizers.Adam(learning_rate=0.001),
    metrics=[tf.keras.metrics.RootMeanSquaredError()])

"""Melakukan proses training dengan `fit` dengan batch size sebesar 64 dan 25 epochs."""

# Memulai training

history = model.fit(
    x = x_train,
    y = y_train,
    batch_size = 64,
    epochs = 25,
    validation_data = (x_val, y_val)
)

"""Memvisualisasikan hasil dari proses training yang berlangsung. Dari visualisasi proses training model di atas cukup smooth dan model konvergen pada epochs sekitar 25. Dari proses ini, kita memperoleh nilai error akhir sebesar sekitar 0.19 dan error pada data validasi sebesar 0.20."""

plt.plot(history.history['root_mean_squared_error'])
plt.plot(history.history['val_root_mean_squared_error'])
plt.title('model_metrics')
plt.ylabel('root_mean_squared_error')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

"""### Rekomendasi Movie"""

movie_df = movie_fix
df = pd.read_csv('movielens-dataset/ratings.csv')


user_id = df.userId.sample(1).iloc[0]
movie_watched_by_user = df[df.userId == user_id]


movie_not_watched = movie_df[~movie_df['id'].isin(movie_watched_by_user.movieId.values)]['id']
movie_not_watched = list(
    set(movie_not_watched)
    .intersection(set(movie_encoded.keys()))
)

movie_not_watched = [[movie_encoded.get(x)] for x in movie_not_watched]
user_encoder = user_encoded.get(user_id)
user_movie_array = np.hstack(
    ([[user_encoder]] * len(movie_not_watched), movie_not_watched)
)

"""Digunakan `.predict()` untuk mendapatkan rekomendasi movie"""

ratings = model.predict(user_movie_array).flatten()

top_ratings_indices = ratings.argsort()[-10:][::-1]
recommended_movie_ids = [
    to_movie.get(movie_not_watched[x][0]) for x in top_ratings_indices
]

print('Showing recommendations for users: {}'.format(user_id))
print('===' * 9)
print('movie with high ratings from user')
print('----' * 8)

top_movie_user = (
    movie_watched_by_user.sort_values(
        by = 'rating',
        ascending=False
    )
    .head(5)
    .movieId.values
)

movie_df_rows = movie_df[movie_df['id'].isin(top_movie_user)]
for row in movie_df_rows.itertuples():
    print(row.title, ':', row.genre)

print('----' * 8)
print('Top 10 movie recommendation')
print('----' * 8)

recommended_movie = movie_df[movie_df['id'].isin(recommended_movie_ids)]
for row in recommended_movie.itertuples():
    print(row.title, ':', row.genre)

"""Dari hasil rekomendasi di atas, diketahui bahwa rekomendasi paling tinggi adalah movie dengan genre comedy. Kemudian, top 10 movie yang direkomendasikan oleh sistem adalah Animasi, Fantasi, Sci-fi, dan Thriller."""

